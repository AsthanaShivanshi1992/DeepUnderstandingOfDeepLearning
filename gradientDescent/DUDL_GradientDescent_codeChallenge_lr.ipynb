{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bhWV8oes-wKR"
   },
   "source": [
    "# COURSE: A deep understanding of deep learning\n",
    "## SECTION: Gradient descent\n",
    "### LECTURE: CodeChallenge: dynamic learning rates\n",
    "#### TEACHER: Mike X Cohen, sincxpress.com\n",
    "##### COURSE URL: udemy.com/course/deeplearning_x/?couponCode=202401"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "JL_0UKJOj1YP"
   },
   "outputs": [],
   "source": [
    "# import all necessary modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import matplotlib_inline.backend_inline\n",
    "matplotlib_inline.backend_inline.set_matplotlib_formats('svg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OeYMLgEvZY1X"
   },
   "source": [
    "# Create the function and its derivative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "YwTBzVJsoKbg"
   },
   "outputs": [],
   "source": [
    "# define a range for x\n",
    "x = np.linspace(-2,2,2001)\n",
    "\n",
    "# function (as a function)\n",
    "def fx(x):\n",
    "  return 3*x**2 - 3*x + 4\n",
    "\n",
    "# derivative function\n",
    "def deriv(x):\n",
    "  return 6*x - 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nWC2T55ovJ-4"
   },
   "source": [
    "### G.D. using a fixed learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "yB8dmH-nvJ_D"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/wt5p5ysx6zb5xqh008fhym3w0000gp/T/ipykernel_4029/2662042246.py:31: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  modelparamsFixed[i,0] = localmin\n",
      "/var/folders/3n/wt5p5ysx6zb5xqh008fhym3w0000gp/T/ipykernel_4029/2662042246.py:32: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  modelparamsFixed[i,1] = grad\n"
     ]
    }
   ],
   "source": [
    "# random starting point\n",
    "localmin = np.random.choice(x,1)\n",
    "initval = localmin[:] # store the initial value\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "training_epochs = 50\n",
    "\n",
    "'''Structure of the Array:\n",
    "The array has training_epochs rows, where each row corresponds to a single epoch (iteration) of the training process.\n",
    "The number 3 represents the number of columns in each row, meaning that for each epoch, three different values are being stored.\n",
    "Purpose of Each Column:\n",
    "modelparamsFixed[i, 0]: This column stores the updated parameter value (localmin) at the end of each epoch.\n",
    "modelparamsFixed[i, 1]: This column stores the computed gradient (grad) at that epoch.\n",
    "modelparamsFixed[i, 2]: This column stores the learning rate (lr) used during that epoch.'''\n",
    "\n",
    "# run through training and store all the results\n",
    "modelparamsFixed = np.zeros((training_epochs,3)) #four columns\n",
    "for i in range(training_epochs):\n",
    "  \n",
    "  # compute gradient\n",
    "  grad = deriv(localmin)\n",
    "\n",
    "  # non-adaptive learning rate\n",
    "  lr = learning_rate\n",
    "\n",
    "  # update parameter according to g.d.\n",
    "  localmin = localmin - lr*grad\n",
    "\n",
    "  # store the parameters\n",
    "  modelparamsFixed[i,0] = localmin\n",
    "  modelparamsFixed[i,1] = grad\n",
    "  modelparamsFixed[i,2] = lr\n",
    "\n",
    "  #So in the end we have a 50*3 matrix with columns training epoch, local min, grad,lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G24R3zyh6XWA"
   },
   "source": [
    "### G.D. using a gradient-based learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "M22aVI6xVIbk"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/wt5p5ysx6zb5xqh008fhym3w0000gp/T/ipykernel_4029/3428914361.py:26: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  modelparamsGrad[i,0] = localmin\n",
      "/var/folders/3n/wt5p5ysx6zb5xqh008fhym3w0000gp/T/ipykernel_4029/3428914361.py:27: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  modelparamsGrad[i,1] = grad\n",
      "/var/folders/3n/wt5p5ysx6zb5xqh008fhym3w0000gp/T/ipykernel_4029/3428914361.py:28: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  modelparamsGrad[i,2] = lr\n"
     ]
    }
   ],
   "source": [
    "# random starting point\n",
    "localmin = np.random.choice(x,1)\n",
    "initval = localmin[:] # store the initial value\n",
    "\n",
    "# learning parameters\n",
    "learning_rate = .01\n",
    "training_epochs = 50\n",
    "\n",
    "# run through training and store all the results\n",
    "modelparamsGrad = np.zeros((training_epochs,3))\n",
    "for i in range(training_epochs):\n",
    "  \n",
    "  # compute gradient\n",
    "  grad = deriv(localmin)\n",
    "\n",
    "  # adapt the learning rate according to the gradient\n",
    "  #When grad is large we are away from the min, so lr is high, model takes large steps\n",
    "  #As we reach the minimum, lr gets smaller, model takes smaller steps due to mutipication with grad and adopting an adaptive learning rate\n",
    "\n",
    "  lr = learning_rate*np.abs(grad) #why np.abs? Becasue in case grad is negative, we dont want the code to start applyng gradient ascent\n",
    "\n",
    "  # update parameter according to g.d.\n",
    "  localmin = localmin - lr*grad\n",
    "\n",
    "  # store the parameters\n",
    "  modelparamsGrad[i,0] = localmin\n",
    "  modelparamsGrad[i,1] = grad\n",
    "  modelparamsGrad[i,2] = lr\n",
    "\n",
    "  #Learning rate, gradient and localmin for each epoch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "49VT4I5B6rhQ"
   },
   "source": [
    "### G.D. using a time-based learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "X6OWQULR30oV"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/3n/wt5p5ysx6zb5xqh008fhym3w0000gp/T/ipykernel_4029/287006009.py:11: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  modelparamsTime[i,0] = localmin\n",
      "/var/folders/3n/wt5p5ysx6zb5xqh008fhym3w0000gp/T/ipykernel_4029/287006009.py:12: DeprecationWarning: Conversion of an array with ndim > 0 to a scalar is deprecated, and will error in future. Ensure you extract a single element from your array before performing this operation. (Deprecated NumPy 1.25.)\n",
      "  modelparamsTime[i,1] = grad\n"
     ]
    }
   ],
   "source": [
    "# redefine parameters\n",
    "learning_rate = .1\n",
    "localmin = initval\n",
    "\n",
    "# run through training and store all the results\n",
    "modelparamsTime = np.zeros((training_epochs,3))\n",
    "for i in range(training_epochs):\n",
    "  grad = deriv(localmin)\n",
    "  lr = learning_rate*(1-(i+1)/training_epochs)\n",
    "  localmin = localmin - lr*grad\n",
    "  modelparamsTime[i,0] = localmin\n",
    "  modelparamsTime[i,1] = grad\n",
    "  modelparamsTime[i,2] = lr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XEYWC-m36yYX"
   },
   "source": [
    "### Plot the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AInqnFtkVIeb"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,3,figsize=(10,3))\n",
    "\n",
    "# generate the plots\n",
    "for i in range(3):\n",
    "  ax[i].plot(modelparamsFixed[:,i],'o-',markerfacecolor='w')\n",
    "  ax[i].plot(modelparamsGrad[:,i],'o-',markerfacecolor='w')\n",
    "  ax[i].plot(modelparamsTime[:,i],'o-',markerfacecolor='w')\n",
    "  ax[i].set_xlabel('Iteration')\n",
    "\n",
    "ax[0].set_ylabel('Local minimum')\n",
    "ax[1].set_ylabel('Derivative')\n",
    "ax[2].set_ylabel('Learning rate')\n",
    "ax[2].legend(['Fixed l.r.','Grad-based l.r.','Time-based l.r.'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QvGE12fEiui7"
   },
   "source": [
    "# Additional explorations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "m54Y_kYXiwO9"
   },
   "outputs": [],
   "source": [
    "# 1) Change the initial learning rate in the \"time\" experiment from .1 to .01. Do you still reach the same conclusion that\n",
    "#    dynamic learning rates are better than a fixed learning rate?\n",
    "# \n",
    "# 2) Compute the average of all time-based learning rates (see variable 'modelparamsTime'). Next, replace the fixed \n",
    "#    learning rate with the average over all dynamic learning rates. How does that affect the model's performance?\n",
    "# \n",
    "# 3) Going back to the original code (without the modifications above), you saw that the fixed learning rate model didn't\n",
    "#    get to the same local minimum. What happens if you increase the number of training epochs from 50 to 500? Does that \n",
    "#    improve the situation, and what does that tell you about the relationship between learning rate and training epochs?\n",
    "# \n",
    "# 4) The code here initializes the starting value as a random number, which will differ for each learning rate method.\n",
    "#    Is that appropriate or inappropriate for this experiment? Why? Change the code so that the starting value is the\n",
    "#    same for all three learning rate models.\n",
    "# "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPJEKg34uwM2jsl7450q0mG",
   "provenance": [
    {
     "file_id": "1AKaKETDfNqVEFzv5BnYGSjHfaPtFpH4x",
     "timestamp": 1617036007367
    },
    {
     "file_id": "1qFigOOWXcSNyA6hPpZnF-QqhFEB1e-hy",
     "timestamp": 1597210805829
    },
    {
     "file_id": "1kGRo0g3UXxXpJuQSEtpKjEGA1Vxbaz8S",
     "timestamp": 1597128018290
    },
    {
     "file_id": "1U4oG0A3DFC-XBWhvecYeA3YYReqHpShX",
     "timestamp": 1594575042741
    }
   ]
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
